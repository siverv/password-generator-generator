<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Password Generator Generator</title>
    <link rel="stylesheet" href="./src/main.css"/>
    <script src="./src/components/WcTabPanel.js"></script>
</head>
<body>
    <main>
        <noscript>
            JavaScript required to generate any generators, with the one and only
        </noscript>
        <header>
            <h1>
                Password Generator Generator
            </h1>
            <a href="https://github.com/siverv/password-generator-generator" class="github-link">
                GitHub
            </a>
            <h2>State machines and symbolic dynamics and entropy, oh my!</h2>
        </header>

        

        <section class="specification-section">
            <wc-tab-panel id="specification-panel" search-param="main-tabs">


                <h3 slot="tab">
                    generator generator
                </h3>
                <section slot="content">
                    <label>
                        Presets:
                        <select id="predefinedSelect">
                        </select>
                    </label>
                    <div>
                        <textarea id="specification" name="specification">
                        </textarea>
                    </div>
                    <div>
                        <button type="submit" id="run">
                            Generate Generator
                        </button>
                        <pre id="error-log">
                        </pre>
                    </div>
                    <!-- <div>
                        <label>
                            Name:
                            <input type="text">
                        </label>
                        <button>
                            save
                        </button>
                    </div> -->
                </section>


                <h3 slot="tab">
                    about
                </h3>
                <section slot="content" class="markdown">
                    


<h1>
<a id="user-content-password-generator-generator" class="anchor" href="#password-generator-generator" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Password Generator Generator</h1>
<p>This is a script and a small web-app to generate password generators using finite state machines (or shifts of finite type) with potentially non-uniform probability of different emissions and support for non-character emissions.</p>
<p>The app produces four kinds of password generators:</p>
<ul>
<li>A self-contained JavaScript function</li>
<li>A bookmarklet alerting a newly generated password</li>
<li>An HTML-page with "generate"- and "copy"-button</li>
<li>A data-URI containing the aforementioned HTML-page.</li>
</ul>
<p>The generators use <code>window.crypto.getRandomValues</code> for randomness, and entropy is calculated using <a href="./THEORY.md">min-entropy</a></p>
<h2>
<a id="user-content-entropy-and-pronouncible-passwords" class="anchor" href="#entropy-and-pronouncible-passwords" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy and Pronouncible Passwords</h2>
<p>How secure is an automatically generated pronouncible password? Do you need 16, 32, or perhaps even 64 characters to get close to the more secure <code>B&amp;QlWYrj#$3A-&amp;4P</code>-type passwords. The application uses min-entropy to calculate the entropy of the resulting adjacency matrix from the states. From this entropy calculation, one may judge whether a given password scheme is strong enough for the desired use.</p>
<h2>
<a id="user-content-correct-horses-and-their-battery-staples" class="anchor" href="#correct-horses-and-their-battery-staples" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Correct Horses, and their Battery Staples</h2>
<p>One may use non-character tokens to replicate the classic <a href="https://xkcd.com/936/" rel="nofollow">Correct Horse Battery Staple</a>, though a single state and uniform probability makes both the generator and its entropy is wholly dependent on the wordlist. For example, xkcd's calculation example uses <code>2^11=2048</code> words.</p>
<p>More states generally means less entropy, but since the CHBS-state uses many characters per token to achieve its 11 bits of entropy, adding a few more states can increase entropy-per-character while still maintaining some of its memorability.</p>
<pre><code>groups:
    words:
        - Correct
        - Horse
        - Battery
        - Staple
        - ...
    fair-dice-roll: "4"

states:
    -   window: [fair-dice-roll]
        emit: words + 5 * fair-dice-roll
    -   window: [words]
        emit: fair-dice-roll
</code></pre>
<h2>
<a id="user-content-the-specification-format" class="anchor" href="#the-specification-format" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Specification Format</h2>
<p>The specification is a yml-formatted text containing two fields: <code>groups</code> and <code>states</code>. <code>groups</code> is an optional field.</p>
<p>Example of a simple syllabetical password generator:</p>
<pre><code>groups:
    v: aeiouy
    c: bcdfghjklmnpqrstvwxz

states:
    -   name: consonant
        window: [c]
        emit: v
    -   name: vowel
        window: [v]
        emit: c

</code></pre>
<h3>
<a id="user-content-groups" class="anchor" href="#groups" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Groups</h3>
<p>The optional <code>groups</code>-field contains a simple map of group-names to token sets. If the token-set is a simple string, each character is treated as a token. If the token-set is an array, each item in the array is considered as a token.</p>
<h3>
<a id="user-content-states" class="anchor" href="#states" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>States</h3>
<p>The <code>states</code>-field is a list of states, with a <code>window</code>-, <code>emit</code>- and (optionally) <code>name</code>-field.</p>
<p><code>window</code> is an array of tokens that need to match in the output. The window (in short-hand) form <code>[a,b,c]</code> matches any output [...,a,b,c]. If there is a group named <code>a</code>, <code>b</code>, or <code>c</code>, that group will be used instead of the letters, otherwise each character will be treated as a token here. The set of possibilities for each window can also be objects with a field <code>groups</code> or <code>tokens</code> if one wants to be explicit.</p>
<p><code>emit</code> is an array of objects <code>{weight: number, tokens: Set&lt;string&gt;}</code> (or with a <code>groups</code>-field instead of <code>tokens</code>-field). In shorthand-form, one may write <code>a + n * b + m * c</code>, in which <code>a</code>, <code>b</code>, and <code>c</code> are groups names or tokenstrings, and <code>n</code> and <code>m</code> are numbers representing weight.</p>
<h3>
<a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>
<ul>
<li>My knowledge of symbolic dynamics can be credited to Douglas Lind, who wrote <a href="https://faculty.washington.edu/lind/symbolic-book/" rel="nofollow">The Book</a>, and held a wonderful Danish-Norwegian master-class on the subject back in 2015.</li>
<li>Inspiration from the LastPass pronouncible password generator on iOS, for being pronouncible but not including any measure of entropy or recommendations about length.</li>
<li>
<a href="https://github.com/pieroxy/lz-string/">lz-string</a> to compress the state before storing it in a hash.</li>
<li>
<a href="https://github.com/nodeca/js-yaml">js-yaml</a> for parsing the specification format</li>
<li>The <code>wc-tab-panel</code> web component is based on the code from <a href="https://github.com/ndesmic/wc-lib">ndesmic's wc-lib</a>
</li>
</ul>






                </section>


                <h3 slot="tab">
                    theory
                </h3>
                <section slot="content" class="markdown math">
                    


<h1>
<a id="user-content-entropy-of-password-generators" class="anchor" href="#entropy-of-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Password Generators</h1>
<h2>
<a id="user-content-tldr" class="anchor" href="#tldr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TL;DR</h2>
<p>The entropy in use is <a href="https://en.wikipedia.org/wiki/Min-entropy" rel="nofollow">min-entropy</a>, giving a pessimistic measure of the entropy which is <a href="https://eprint.iacr.org/2011/659.pdf" rel="nofollow">the desired outcome for security and cryptographic concerns</a></p>
<blockquote>
<p><g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> The following is a personal exploration around the theory of entropy and should probably not be cited directly.</p>
</blockquote>
<ol>
<li><a href="#user-content-entropy-of-standard-password-generators">Entropy of Standard Password Generators</a></li>
<li><a href="#user-content-entropy-of-stateful-password-generators">Entropy of Stateful Password Generators</a></li>
<li><a href="#user-content-entropy-of-non-irreducible-password-generators">Entropy of Non-Irreducible Password Generators</a></li>
<li><a href="#user-content-entropy-of-potentially-non-uniform-password-generators">Entropy of Potentially Non-Uniform Password Generators</a></li>
<li><a href="#user-content-entropy-of-stochastic-password-generators">Entropy of Stochastic Password Generators</a></li>
<li><a href="#user-content-entropy-of-obscure-password-generators">Entropy of Obscure Password Generators</a></li>
</ol>
<h2>
<a id="user-content-entropy-of-standard-password-generators" class="anchor" href="#entropy-of-standard-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Standard Password Generators</h2>
<p>A perfect adversary knows exactly how you generated your password, even how the random numbers were generated up to and including the time of day, mac-address of your machine, and the sensory input of your mouse. They are only limited by the rate which they can attempt different passwords.</p>
<p>The standard secure password generators would use an alphabet $\mathcal{A} = \{a,b,c...\}$ with uniform probability of each letter, and a cryptographically secure random number generator. The adversary knows your password has $n$ letters, and so they have $B_n = |\mathcal{A}|^n$ different passwords to try and no additional hints to judge which is more likely.</p>
<p>With a password generator $G_{\mathcal{A}}$, the number of possibilities $B_n$ tend to grow exponentially with regards to the password-length $n$, and the base of this exponential growth then gives a measure of the _entropy_. In particular, the entropy is usually defined as relative to base 2, so that the exponential growth can be written in the form $2^{cn}$ for some $c$. This $c$ is then the _bits of entropy_ inherent in the generator, which we will denote as $h(G_\mathcal{A})$ (historical note: _h_ is for entropy, due to capital _eta_ being written $\Eta$, which looks similar to latin _H_)</p>
<p>$$h(G_\mathcal{A}) = \lim_{n\to\infty} \frac{1}{n} \log_2 B_n = \log_2 |\mathcal{A}|$$</p>
<p>Thus the entropy of a standard password generators can be considered wholly dependent upon the number of letters available.</p>
<p>Now, how many attempts do they have to try before it starts becoming more likely that they have succeeded than not? They have chosen an optimal sequence of guesses, ${g_1, g_2, g_3, ..., g_{B_n}}$, so the number of guesses they had to make if the right password was $g_i$ is then $G_{g_i}=i$. The expected number of guesses before they succeed guessing your password $X$ is then</p>
<p>$$E(G_X) = \sum_{i=1}^{B_n} P(X = g_i) E(G_X | X = g_i) = \frac{1}{B_n}\sum_{i=1}^{B_n}i = \frac{B_n + 1}{2}$$</p>
<p>They are expected to try at least half the possible passwords before they succeed. Written in terms of the entropy, the formula then becomes</p>
<p>$$ E(G_X) = \frac{2^{h(G_\mathcal{A})n} + 1}{2} &gt; 2^{h(G_\mathcal{A})n - 1} $$</p>
<p>Note that this method of calculating expected guesses only holds for password generators with a uniform probability.</p>
<h2>
<a id="user-content-entropy-of-stateful-password-generators" class="anchor" href="#entropy-of-stateful-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Stateful Password Generators</h2>
<p>Sometimes a perfect password generator is not perfect for _you_. Perhaps you would like a password that is easier to remember, or has a particular pattern to it. Sometimes you want _stateful_ generators that has a slight chance of a number, but never more than one at a time.</p>
<p>Each choice here would reduce the entropy and require longer passwords for the same amount of security, but the trade-off can be worth it. The question then is how to find the entropy of your bespoke generator so that you are not unintentionally weakening your security for convenience.</p>
<p>Let's say your generator has two states: <code>vowel</code> and <code>consonant</code>. In the <code>vowel</code>-state, it emits a consonant and changes the state to the <code>consonant</code>-state, and vice versa. This generator $G_\mathcal{S}$ can be visualized as a graph:</p>
<pre><code>            aeiouy
+-------+-----------------------&gt; +-----------+
| VOWEL |                         | CONSONANT |
+-------+ &lt;-----------------------+-----------+
            bcdfghjklmnpqrstvwxz
        
</code></pre>
<p>This system would emit an infinite sequence of symbols <code>...azucumomozyhisulujewibokydacezunanubus...</code> and to make this a password generator, we just cut out a random section of $n$ letters. The study of such infinite sequences of symbols is a part of Symbolic Dynamics, and the space of all possible infinite sequences of a given pattern is known as a Shift Space. From the study of shift spaces, we can find a series of theorems and proposition that will help us determine the entropy of stateful password generators.</p>
<p>But first, let's start with our existing definition of entropy, using $B_n(G_\mathcal S)$ the number of possible passwords of length $n$:</p>
<p>$$ h(G_\mathcal{S}) = - \lim_{n\to\infty} \frac{1}{n}\log_2 B_n({G_\mathcal{S}}) $$</p>
<p>To find $B_n(G_\mathcal S)$, it can be useful to look upon the adjacency matrix of our state diagram</p>
<p>$$ A_{S} = \begin{bmatrix}
0 &amp; aeiouy \\
bcdfghjklmnpqrstvwxz &amp; 0
\end{bmatrix} = \begin{bmatrix}
0 &amp; 6 \\
20 &amp; 0
\end{bmatrix} $$</p>
<p>The number of 2-letter passwords of the starting in state $i$ and ending in state $j$ is $\sum_{k\in\mathcal S} (A_{\mathcal S})_{ik}(A_{\mathcal S})_{kj}$, otherwise known as be $(A_{\mathcal S}^2)_{ij}$. This holds in general, and $(A_{\mathcal S}^n)_{ij}$ is is excatly the number of passwords from starting in state $i$ and ending in state $j$ and so we get</p>
<p>$$ B_n(G_\mathcal S) = \sum_{ij} (A_\mathcal S^n)_{ij}$$</p>
<p>While calculating this directly is possible, there is a large amount of existing theory available for us, especially if our graph is _irreducible_. An _irreducible_ graph is one in which there exists a path starting at state $i$, ending in state $j$, for any two $i,j$.</p>
<p><a href="https://faculty.washington.edu/lind/symbolic-book/" rel="nofollow">Perron-Frobenius Theory</a> tells us that any irreducible matrix $A \neq 0$ have a perron-eigenvalue $\lambda_A &gt; 0$ such that any other eigenvalue $\mu$, $|\mu| \leq \lambda_A$.</p>
<p>Furthermore, <a href="https://faculty.washington.edu/lind/symbolic-book/" rel="nofollow">an unnamed theorem</a> tells us that for any irreducible _right resolving_ graph $G$ with adjancency matrix $A$, we have that</p>
<p>$$ h(G) = \log_2 \lambda_A $$</p>
<p>A _right resolving_ graph is a graph with labeled edges, in which all edges out of a given vertex have different labels. <a href="https://faculty.washington.edu/lind/symbolic-book/" rel="nofollow">A different theorem</a> actually tells us that all "password generators of finite memory" (also known as _sofic shifts_) can be represented with a right resolving graph.</p>
<p>The password generators that we can generate with the the specification format as of the time of writing, belong to an even more restrictive class of shifts, namely _shifts of finite type_ (SFT). These are the spaces that have a finite set of "forbidden" sequences, and our current <code>window</code>-condition for state change naturally generates such a set. This gives us an easier condition, as <a href="https://faculty.washington.edu/lind/symbolic-book/" rel="nofollow">for an irreducible graph $G$ (with adjacency matrix $A_G$) representing a SFT, we have $h(G) = \log_2 \lambda_{A_G}$</a>.</p>
<p>Now the question remains to make our password generators be irreducible, and the entropy is can be easily calculated.</p>
<h2>
<a id="user-content-entropy-of-non-irreducible-password-generators" class="anchor" href="#entropy-of-non-irreducible-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Non-Irreducible Password Generators</h2>
<ul>
<li>Breaking down a non-irreducible password generator $A$ into irreducible components $A_i$</li>
<li>Theorems showing that $\lambda_A = \max_i \lambda_{A_i}$, and that $h(G_A) = \lambda_A$ still holds for _shifts_ in particular</li>
<li>Arguing that for password generators in particular, being finite, the entropy is much more affected by the initial state than any infinitely running shift.</li>
<li>Consider introducing "run-length" dependent entropy for password generators, because the shift-entropy is "infinite run-length".</li>
</ul>
<h2>
<a id="user-content-entropy-of-potentially-non-uniform-password-generators" class="anchor" href="#entropy-of-potentially-non-uniform-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Potentially Non-Uniform Password Generators</h2>
<p>We have now mostly dealt with entropy of password generators as if they are always equal to the entropy of the shift space from which the passwords are gathered. This is not necessarily the case, and must be investigated.</p>
<p>In particular, a difference between shifts and password generators is that shifts concerns themselves mostly about which (infinite) sequences are possible, while password generators need to think about which (finite) sequences are probable. While we can calculate a measure of entropy based on the number of passwords generated $B_n$, it would be more helpful to calculate the entropy as it changes for each transition between the states, how much _information_ is added by each step. This would allow us to expand our definition of entropy into non-uniform probabilities of passwords.</p>
<p>Based on the work of <a href="https://en.wikipedia.org/wiki/Claude_Shannon" rel="nofollow">Claude Shannon</a> there is generally four axioms that encompass what it means to measure the information value $I$ of an event. <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_note-10" rel="nofollow">To quote Wikipedia</a></p>
<blockquote>
<ol>
<li>
<p>$I(p)$ is monotonically decreasing in p: an increase in the probability of an event decreases the information from an observed event, and vice versa.</p>
</li>
<li>
<p>$I(p) \geq 0$: information is a non-negative quantity.</p>
</li>
<li>
<p>$I(1) = 0$: events that always occur do not communicate information.</p>
</li>
<li>
<p>$I(p1, p2) = I(p1) + I(p2)$: the information learned from independent events is the sum of the information learned from each event.</p>
</li>
</ol>
<p>...</p>
<p>Shannon discovered that a suitable choice of $I$ is given by:</p>
<p>$$I(p) = \log(\frac{1}{p}) = -\log(p)$$</p>
<p>In fact, the only possible values of I are $I(u) = k\log u$ for $k&lt;0$.</p>
</blockquote>
<p>In particular,  entropy $\Eta$ is generally defined as the _expected value_ of information content, which using this formula gives us:</p>
<p>$$ \Eta(X) = E[I(X)] = - \sum_{x_i\in X} P(x_i)I(x_i) = - \sum_{x_i\in X} P(x_i)\log_b(P(x_i)) $$</p>
<p>The log-base $b$ is usually chosen as $2$, giving us _bits of entropy_.</p>
<p>We may also show that this definition is consistent with our earlier definition of entropy $\lim_{n\to\infty} \frac{1}{n} \log_2 B_n$ for certain groups of generators. Given a password generator $G$, the probability for a given password $p$ of length $n$ is $1/B_n$, which gives us:</p>
<p>$$ \Eta(G_n) = - \sum_{p\in G_n} \frac{1}{B_n}\log_2(\frac{1}{B_n}) = \log_b{B_n} $$</p>
<p>Therefore the entropy "per character" in the generator goes to $h(G)$ as $n\to\infty$. As long as we have _right resolving_ matrices, with uniform probability between the edges, each password of a given length remains of uniform probability and we can continue to use $\lambda_A$ to calculate the entropy as the two definitions coincide. This new definition gives us a way to adjust the probabilities of edges while still being able to judge entropy, as well as feel more certain in calculating the entropy of $n$-length passwords using $B_n = \Sigma_{ij} A^n_{ij}$</p>
<h2>
<a id="user-content-entropy-of-stochastic-password-generators" class="anchor" href="#entropy-of-stochastic-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Stochastic Password Generators</h2>
<p>A syllabetical password generator is readable and pronouncible, and generally just fine, but what if we want more? What if instead we want to base our passwords around bigrams, in which the edges are purposely _non-uniform_ in their distribution? Perhaps we have a strong preference for the vowel <code>a</code> as opposed to <code>y</code>? Now let's introduce probability to our edges, and explore how that affects the entropy.</p>
<p>There is established theory about this if we turn to Markov models, but they concern themselve smostly with the probability of the destination, and not the manner in which you arrive, and so we need to modify our graph in a particular manner. But first, let's ensure we all are operating on the definition and notation.</p>
<p>A topological Markov chain is a pair $(P,\pi)$ of a _stochastic transition matrix_ $P$ and its _stationary probability vector_ $\pi$, that is _compatible_ with a given _shift of finite type_ (ie. $\pi_{ij} = 0$ whenever $A_{ij} = 0$).</p>
<p>In the earlier sections, we defined the <a href="https://en.wikipedia.org/wiki/Topological_entropy" rel="nofollow">topological entropy</a> on subshifts  measuring variety, and the Shannon entropy on uncertain events measuring information content. For Markov models in general it more common to operate with the <a href="https://en.wikipedia.org/wiki/Measure-preserving_dynamical_system#Measure-theoretic_entropy" rel="nofollow">Kolmogorov-Sinai entropy</a> of the <a href="https://en.wikipedia.org/wiki/Subshift_of_finite_type#Measure" rel="nofollow">Markov measure</a>, but we are in a cryptographical context, and so there is another entropy that might be of more interest: <code>min-entropy</code>.</p>
<p>In a cryptographical context we are more concerned about being _secure_ rather than _right_, and so the <code>min-entropy</code> can give us a better baseline on which we can build our tolerances for insecurity.</p>
<p>The general definition of min-entropy is not as useful to us:</p>
<p>$$H_\infty(p) = - \log_2 max_x p (x) = min_x (-\log_2 p(x)) $$</p>
<p>This would require iterating over all possible passwords, but for Markov models, <a href="https://eprint.iacr.org/2011/659.pdf" rel="nofollow">there is a better way</a>:</p>
<p>$$H_\infty((\pi,P)_n) = -\log_2\left(\pi\odot \underbrace{P\odot P \odot ... \odot P}_{n-1} \odot \vec 1 \right) $$</p>
<p>Here $\odot$ is defined as a matrix multiplication, but reducing the dot-product with $\max$ instead of $\sum$, and $\vec 1_i = 1$ for all $i$.</p>
<p>Now the only remaining problem is that this is defined for "proper" markov models, but for computational simplicity the password generator this page is written for is based around the concept of "emission sets". With $(G, A)$ be our right resolving graph and associated adjacency matrix for our model, with edge-labels in $\mathcal A$. We define the emission sets of $\mathbb A_i$ as the disjoint sets $A_{i,x} \subset \mathcal{A_{i}}$ such that any label $\alpha_x\in A_{i,x}$ have the same probability $p_x$ and ends up in the same state $j_x$. We state $A_{ij,x}\subset \mathcal A_{ij}$ for the subsets ending in j</p>
<p>Let $K_ij$ be the set of states in which both $A_{ik}\neq 0$ and $A_{kj}\neq 0$</p>
<p>$$
\begin{array}{rl}
H_{i\to_2j} &amp;= -\max_{k\in K_{ij},x\in A_{ik},y\in A_{kj}} \log_2 (p_{i,x}p_{k,y}) \\
&amp;= -\max_{k\in K_{ij}} \max_{x\in A_{ik}} \log_2 p_{i,x} + \max_{y\in A_{kj}} \log_2 p_{k,y} \\
&amp;= -\max_{k\in K_{ij}} (\max_{A_x\subset \mathcal A_{ik}} \log_2 p_x + \max_{A_y\subset \mathcal A_{kj}} \log_2 p_y)
\end{array}
$$</p>
<p>Let's define $M_{ij} = \max_{x\in A_{ij}} p_x$.</p>
<p>$$
\begin{array}{rl}
(M\odot M)_{ij}
&amp;= \max_{k\in K_{ij}} M_{ik}M_{kj}\\
&amp;= \max_{k\in K_{ij}} \max_{x\in A_{ik}} p_{i,x} \max_{x\in A_{kj}} p_{k,y}\\
&amp;= \max_{k\in K_{ij},x\in A_{ik},y\in A_{kj}} p_{i,x}p_{k,y}
\end{array}
$$</p>
<p>Which gives us $H_{i\to_2j} = -\log_2 M^2_{ij}$. And more in general $H_{ij}(G_n) = (\underbrace{M\odot M \odot ... \odot M}_{n})_{ij}$.</p>
<p>To get our actual entropy we need to have the probability of the initial positions, which for a proper Markov model would be $\pi$. However, since we are using this as a password generator, the "true" initial position is the actual initial position used. In particular, this currently is a rather simple $v_i = \frac{1}{N}$ where $N$ is the number of states. Thus, our result ends as</p>
<p>$$ H_{\tilde\infty}(G_n) = - \log_2 v\odot (\underbrace{M\odot M \odot ... \odot M}_{n})\odot u \leq H_t(G_n)$$</p>
<p>Now this can be more easily worked with, and requires somewhere around $O(N^3n)$ operations to calculate.</p>

<h2>
<a id="user-content-entropy-of-obscure-password-generators" class="anchor" href="#entropy-of-obscure-password-generators" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy of Obscure Password Generators</h2>
<p>While non-standard password generators generally reduces the entropy, and _security by obscurity_ is not generally considered _security_ at all, it often feels that as long as nobody targets _you_ specifically, a custom generator has a virtual increase in entropy. <strong>This is not the case</strong>. Ultimately, the only secure password is one with sufficiently high entropy. One should always strive to avoid any password that has ever been used before, ever. The reason being, at some point a previously used password is going to turn up in a password-leak, and thus be among the first attempted by an automated attacker. A password should strive to be _universally unique_.</p>
<p>Assume that every person ever has created one new (universally unique) password every millisecond for the duration of their lifetime. We want there to be virtually no chance that our generators generate any of these passwords. We have approximately $10^{10}$ people, and and a conservative upper bound of $1000\cdot60\cdot60\cdot24\cdot365\cdot88 = 2.8\cdot 10^{12}$ passwords generated by each person.</p>
<p>Therefore, if all $10^{22}$ passwords were generated with our obscure password generator, we want there to be virtually no chance of any collision what-so-ever. How much entropy do we then need? Well, it depends on what one considers 'virtually no chance', but for sake of argument let's use a winning the lottery. That is, our password generator is "sufficiently secure" if the chance for there being any collision whatsoever is similar to winning a state-lottery, which is about $10^{-7}$</p>
<p>... todo: calculate a proper expression of entropy limits for secure passwords given the above variables.</p>
                        






                </section>


            </wc-tab-panel>
        </section>


        <section class="analyzation-section">
            <wc-tab-panel search-param="aside-tabs">


                <h3 slot="tab">
                    output
                </h3>
                <section slot="content">
                    <fieldset>
                        <legend>
                            Entropy
                        </legend>
                        <div>
                            <ul>
                                <li>
                                    The bits of entropy for this generator is approximately <strong id="entropy-bits">???</strong> per token
                                </li>
                                <li>
                                    This is about equivalent to standard random password with a choice of <strong id="entropy-equiv">???</strong> characters 
                                </li>
                                <li>
                                    To achieve a secure entropy for passwords, you should use at least <strong id="entropy-length">???</strong> tokens from the generator  
                                </li>
                            </ul>
                        </div>
                    </fieldset>
                    <fieldset>
                        <legend>
                            Sample
                            <button id="refresh-sample">
                                ⟳
                            </button>
                        </legend>
                        <pre id="sample-output">
                        </pre>
                    </fieldset>
                    <fieldset class="generator-settings">
                        <legend>
                            Generator Settings
                        </legend>
                        <label>
                            Password length (number of tokens)
                            <input id="desired-length" type="number" min="1" step="1" max="2000" value="32">
                        </label>
                        <p>- or -</p>
                        <label>
                            Bits of Entropy
                            <input id="desired-bits" type="number" min="1" step="1" max="2000" value="64">
                        </label>
                    </fieldset>
                    <wc-tab-panel selectedIndex="3" search-param="output-tabs">
                        <h4 slot="tab">JavaScript Function</h4>
                        <div slot="content" class="code-with-copy">
                            <textarea class="code" id="gen-jsfn"></textarea>
                            <button class="copy-preceding-text">
                                Copy
                            </button>
                        </div>
                        <h4 slot="tab">Bookmarklet</h4>
                        <div slot="content" class="code-with-copy">
                            <textarea class="code compact" id="gen-book"></textarea>
                            <button class="copy-preceding-text">
                                Copy
                            </button>
                            <a href="#" id="gen-book-link">
                                Generate Password
                            </a>
                        </div>
                        <h4 slot="tab">HTML-page</h4>
                        <div slot="content" class="code-with-copy">
                            <textarea class="code" id="gen-html"></textarea>
                            <button class="copy-preceding-text">
                                Copy
                            </button>
                        </div>
                        <h4 slot="tab">Data-URI HTML-page</h4>
                        <div slot="content" class="code-with-copy">
                            <textarea class="code compact" id="gen-data"></textarea>
                            <button class="copy-preceding-text">
                                Copy
                            </button>
                            <a target="_blank" href="#" id="gen-data-link">
                                Link to Password Generator
                            </a>
                        </div>
                    </wc-tab-panel>
                </section>


                <h3 slot="tab">
                    visualization
                </h3>
                <section slot="content">
                    Here be dragons.

                    <div>
                        <i>
                            The intention is to visualize the states and the relations between them using d3. As you might notice, I've not gotten there yet.
                        </i>
                    </div>
                </section>
            </wc-tab-panel>
        </section>

        
        <footer>
            <p>
                <a href="https://github.com/siverv/">siverv</a> 2021
            </p>
        </footer>
    </main>
    <script type="module" src="./src/index.js"></script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
        onload="renderMathInElement(document.querySelector(`.markdown.math`));"></script>
</body>
</html>
